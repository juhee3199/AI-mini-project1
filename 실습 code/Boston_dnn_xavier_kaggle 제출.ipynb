{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Boston_dnn_xavier_kaggle 제출.ipynb","provenance":[{"file_id":"1zG0IF-3hyIFF5laWiNgnetoo6jc_OpUI","timestamp":1592217474560},{"file_id":"1n7Az-_b6wF7hDOh6p2CG7EKzsQRE4xhz","timestamp":1592214114251},{"file_id":"1y9qF3D4vbQVhX_dZCgplRuw_EcgW8Sg2","timestamp":1591114496797},{"file_id":"1CI11CA5otqB7-RsQLKak3LJW3xeCUr9q","timestamp":1590895708437},{"file_id":"1jsP6IkoSZW5roMip53QSPJjiVP6Ej3Zk","timestamp":1590895018573},{"file_id":"1bimNZtout48XzSy7VoWbmdpOHQ-SAkHX","timestamp":1590864591861}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"d3NuyPLviZzA","colab_type":"text"},"source":["### boston 집값 예측\n","target value: 실제 주택 가격"]},{"cell_type":"markdown","metadata":{"id":"jE4dHYRtXLOo","colab_type":"text"},"source":["### NN regressor 모델 설계\n","- classification과 크게 코드차이가 없다.\n","- 차이:\n","1.   train셋과 test 셋 Scaler를 통해 transofrom을 해준다.\n","- 데이터의 분포가 다 다르기때문에 scaler를 맞춰주었다. preprocessing.StandardScaler() 를 사용.\n","2.   손실함수로 MSELoss를 사용한다.\n","3. 평가는 RMSE \n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"m4bGbvfFh-VV","colab_type":"text"},"source":["## baseline을 넘기 위해 내가 한 전략\n","- 1) 1layer 에서 3layer사용 -> 성능이 더 좋지 않음.\n","- 2) 3lyaer + dropout 적용-> **baseline을 넘겼다.**\n","- 3) 2번에서 13>256>1 을 13>512>1 로 변경했다. -> **성능이 조금 더 향상되었다.** \n","- 4) 4layer + dropout -> cost가 전혀 낮아지지 않았다. \n","- 5) 4layer + dropout + 파라미터변경 + hidden layer노드수 변경 -> cost가 전혀 낮아지지 않았다.\n","\n","--->  3)번을 최종으로 제출. **8.73479**"]},{"cell_type":"code","metadata":{"id":"1ufl-SW8BorT","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":339},"executionInfo":{"status":"ok","timestamp":1592214709540,"user_tz":-540,"elapsed":12535,"user":{"displayName":"Juhee","photoUrl":"","userId":"14369088239483395487"}},"outputId":"27895f31-50f1-4469-cf03-2f10fab1a3bd"},"source":["!pip uninstall -y kaggle\n","!pip install --upgrade pip\n","!pip install kaggle==1.5.6"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Found existing installation: kaggle 1.5.6\n","Uninstalling kaggle-1.5.6:\n","  Successfully uninstalled kaggle-1.5.6\n","Requirement already up-to-date: pip in /usr/local/lib/python3.6/dist-packages (20.1.1)\n","Processing /root/.cache/pip/wheels/01/3e/ff/77407ebac3ef71a79b9166a8382aecf88415a0bcbe3c095a01/kaggle-1.5.6-py3-none-any.whl\n","Requirement already satisfied: certifi in /usr/local/lib/python3.6/dist-packages (from kaggle==1.5.6) (2020.4.5.1)\n","Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from kaggle==1.5.6) (1.24.3)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from kaggle==1.5.6) (4.41.1)\n","Requirement already satisfied: python-dateutil in /usr/local/lib/python3.6/dist-packages (from kaggle==1.5.6) (2.8.1)\n","Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from kaggle==1.5.6) (2.23.0)\n","Requirement already satisfied: python-slugify in /usr/local/lib/python3.6/dist-packages (from kaggle==1.5.6) (4.0.0)\n","Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.6/dist-packages (from kaggle==1.5.6) (1.12.0)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->kaggle==1.5.6) (2.9)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->kaggle==1.5.6) (3.0.4)\n","Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.6/dist-packages (from python-slugify->kaggle==1.5.6) (1.3)\n","Installing collected packages: kaggle\n","Successfully installed kaggle-1.5.6\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"IC8Uz2G9B64g","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1592214716589,"user_tz":-540,"elapsed":19527,"user":{"displayName":"Juhee","photoUrl":"","userId":"14369088239483395487"}},"outputId":"3a61b215-f0f7-4ec3-c319-c98cee482208"},"source":["!mkdir -p ~/.kaggle\n","!cp kaggle.json ~/.kaggle\n","!ls -lha kaggle.json\n","!chmod 600 ~/.kaggle/kaggle.json"],"execution_count":null,"outputs":[{"output_type":"stream","text":["-rw-r--r-- 1 root root 64 Jun 15 09:42 kaggle.json\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"puERE7IqCBNs","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":70},"executionInfo":{"status":"ok","timestamp":1592214720217,"user_tz":-540,"elapsed":23107,"user":{"displayName":"Juhee","photoUrl":"","userId":"14369088239483395487"}},"outputId":"36dd41ba-4ee7-4b9c-dec5-0fcff5bd46ad"},"source":["!kaggle competitions download -c 2020aiboston"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Downloading 2020aiboston.zip to /content\n","\r  0% 0.00/14.5k [00:00<?, ?B/s]\n","\r100% 14.5k/14.5k [00:00<00:00, 11.9MB/s]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Jf2XBDZlCEt4","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":88},"executionInfo":{"status":"ok","timestamp":1592214723038,"user_tz":-540,"elapsed":25758,"user":{"displayName":"Juhee","photoUrl":"","userId":"14369088239483395487"}},"outputId":"8230e2ed-bd87-428a-b9d1-7329be25d408"},"source":["!unzip 2020aiboston.zip"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Archive:  2020aiboston.zip\n","  inflating: boston_housing_test.csv  \n","  inflating: boston_housing_train.csv  \n","  inflating: sample_submission.csv   \n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"9EVz_27QV_AJ","colab_type":"code","colab":{}},"source":["train_data=pd.read_csv('boston_housing_train.csv',header=None, usecols=range(1,15))\n","test_data=pd.read_csv('boston_housing_test.csv',header=None, usecols=range(1,14))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Y3bpvTNIRwWl","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1593431169417,"user_tz":-540,"elapsed":4628,"user":{"displayName":"Juhee","photoUrl":"","userId":"14369088239483395487"}}},"source":["import pandas as pd\n","import numpy as np\n","\n","import torch\n","import torchvision.datasets as data\n","import torchvision.transforms as transforms\n","import random\n","\n","from sklearn import preprocessing"],"execution_count":1,"outputs":[]},{"cell_type":"code","metadata":{"id":"QICWFIJUR6PL","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1593431169426,"user_tz":-540,"elapsed":4584,"user":{"displayName":"Juhee","photoUrl":"","userId":"14369088239483395487"}}},"source":["device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","\n","random.seed(777)\n","torch.manual_seed(777)\n","if device == 'cuda':\n","  torch.cuda.manual_seed_all(777)"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"gh_Yz21HVs0V","colab_type":"code","colab":{}},"source":["# 학습 파라미터 설정\n","learning_rate = 0.1\n","training_epochs = 300\n","batch_size = 100\n","\n","drop_prob = 0.3\n","\n","Scaler = preprocessing.StandardScaler()  \n","\n","# 데이터의 분포가 다 다르다. 이를 preprocessing.StandardScaler()를 이용해  Scaler를 맞춰주었다. "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"w94IlzOdC_m2","colab_type":"code","colab":{}},"source":["x_train_data=train_data.loc[:,0:13]\n","y_train_data=train_data.loc[:,14]\n","\n","x_train_data=np.array(x_train_data)\n","y_train_data=np.array(y_train_data)\n","x_train_data = Scaler.fit_transform(x_train_data)\n","\n","x_train_data=torch.FloatTensor(x_train_data)\n","y_train_data=torch.FloatTensor(y_train_data)      # y데이터가 FloatTensor 형이다."],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wBz_3lepEHYX","colab_type":"code","colab":{}},"source":["train_dataset = torch.utils.data.TensorDataset(x_train_data, y_train_data)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-Ae1Q7WXWJcc","colab_type":"code","colab":{}},"source":["# Tensor 데이터셋을 통해 DataLoader를 사용할 수 있다.\n","\n","data_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n","                                          batch_size=batch_size,\n","                                          shuffle=True,\n","                                          drop_last=True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"R0Tk0BBFzOJq","colab_type":"text"},"source":["![대체 텍스트](https://user-images.githubusercontent.com/11758940/83336289-229ec200-a2ed-11ea-9fb8-88a51198e475.png)"]},{"cell_type":"code","metadata":{"id":"ntcrgdljaxST","colab_type":"code","colab":{}},"source":["#linear1 = torch.nn.Linear(13,1,bias=True)\n","\n","linear1 = torch.nn.Linear(13,512,bias=True)\n","linear2 = torch.nn.Linear(512,512,bias=True)\n","linear3 = torch.nn.Linear(512,1,bias=True)\n","\n","relu = torch.nn.ReLU()\n","dropout = torch.nn.Dropout(p=drop_prob)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OUqE8Uibc4xH","colab_type":"text"},"source":["[메뉴얼] https://pytorch.org/docs/stable/nn.init.html#torch.nn.init.xavier_uniform_"]},{"cell_type":"code","metadata":{"id":"MrpNg6XycEWE","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1592217359475,"user_tz":-540,"elapsed":1551,"user":{"displayName":"Juhee","photoUrl":"","userId":"14369088239483395487"}},"outputId":"6d02a194-4c59-4413-c219-8ffe9b0bccb0"},"source":["# Random Init => Xavier Init\n","torch.nn.init.xavier_uniform_(linear1.weight)\n","torch.nn.init.xavier_uniform_(linear2.weight)\n","torch.nn.init.xavier_uniform_(linear3.weight)\n"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Parameter containing:\n","tensor([[-2.5232e-02,  7.4012e-02, -2.6983e-02, -5.8732e-03, -6.8674e-02,\n","          1.0801e-03,  5.5246e-02,  2.1627e-02, -8.1325e-02, -5.7981e-02,\n","         -6.2981e-02,  8.3793e-02, -2.7099e-02,  3.7306e-02,  1.7982e-02,\n","         -3.3500e-02,  4.8161e-02, -6.6684e-02, -7.0474e-02,  5.6616e-02,\n","          2.9783e-02,  9.5706e-02,  5.0730e-02,  7.7860e-03,  6.3860e-02,\n","          9.6119e-02, -1.1522e-02, -9.4184e-02, -6.0141e-02, -3.4349e-02,\n","         -5.1398e-02,  3.2665e-02,  4.7062e-02, -4.7963e-02, -5.3838e-02,\n","         -9.0761e-02, -9.9320e-02, -9.3208e-02, -7.8838e-02, -5.1631e-02,\n","         -2.6706e-02, -9.2609e-02, -2.0587e-02, -1.0519e-01, -4.5574e-02,\n","         -4.2306e-02,  9.4669e-02, -9.1892e-02,  5.2686e-02, -2.9779e-02,\n","          4.8092e-02,  9.9021e-02, -1.0584e-01,  1.0205e-01, -8.8469e-02,\n","          1.7143e-02, -8.6990e-02,  9.9737e-02, -5.3751e-02,  9.1031e-03,\n","          7.2964e-02, -1.5162e-02, -2.4623e-02,  1.6055e-02,  8.7991e-02,\n","         -1.9344e-02,  2.7968e-02, -4.2459e-02,  7.6373e-02, -7.2677e-02,\n","          6.4336e-02, -9.6073e-02, -9.3755e-02,  7.7088e-03, -5.5068e-02,\n","         -1.6935e-03,  9.6355e-02,  6.0870e-02, -1.1958e-02, -3.9691e-02,\n","          5.9120e-03,  1.0347e-01,  3.7180e-02, -1.1708e-02, -8.8440e-02,\n","          6.7994e-02,  4.3832e-02, -7.7118e-02,  5.4879e-02,  7.4479e-02,\n","         -1.5242e-02, -4.2803e-02,  4.7362e-02, -2.8436e-02, -4.0492e-02,\n","          3.6416e-02,  1.2099e-02, -4.6511e-02,  5.3815e-02,  1.5507e-02,\n","         -8.8896e-02,  3.1900e-02, -2.1242e-02,  7.6175e-02,  7.8453e-02,\n","          1.7787e-02,  1.0380e-01, -6.9527e-02, -1.0202e-01, -1.0364e-01,\n","         -5.6656e-02,  6.5627e-03, -1.0261e-01,  6.8438e-02, -8.0967e-02,\n","         -1.7223e-02,  4.3411e-02,  5.0309e-02,  1.0390e-01,  3.5181e-02,\n","         -1.5251e-02, -1.1738e-02,  9.4656e-02, -3.3748e-02,  1.0803e-01,\n","         -2.6391e-02, -6.7872e-02,  1.7466e-02, -3.8234e-02, -5.8189e-02,\n","         -5.9472e-02, -9.4656e-02,  2.9984e-02,  4.8138e-02, -8.5565e-02,\n","         -6.4585e-02, -6.8706e-02,  2.7508e-02, -3.6461e-02, -8.1573e-02,\n","          7.1006e-02,  1.0707e-01,  9.0915e-02, -3.4531e-02,  8.8757e-02,\n","         -9.0067e-02,  9.3251e-02, -6.7419e-02,  8.3542e-02, -9.4591e-02,\n","          8.1520e-02,  3.4484e-03,  2.1811e-02, -6.5103e-02, -8.9282e-02,\n","         -3.6296e-03,  7.6144e-02, -1.4905e-03,  1.0695e-01, -5.8543e-02,\n","         -5.4473e-03,  1.0597e-01, -7.8839e-02,  2.6226e-02, -4.8727e-02,\n","         -6.7265e-02,  3.6193e-02, -9.0410e-03, -5.9460e-02, -5.6296e-02,\n","          1.0053e-01,  9.9031e-02, -4.4991e-02,  1.0612e-01, -7.9442e-02,\n","         -1.0496e-01, -2.9587e-02, -2.3218e-02, -6.9341e-03, -2.5369e-02,\n","         -1.0497e-01, -4.6186e-03, -8.5642e-02, -1.4487e-02, -2.6510e-02,\n","         -2.9921e-02,  1.6450e-02, -1.0018e-01, -1.2061e-02, -9.5589e-02,\n","          4.3399e-02, -1.5807e-02, -1.0351e-01,  1.2650e-02,  1.0116e-01,\n","          4.3318e-02, -3.2520e-02, -9.3445e-02,  2.7988e-03, -4.5596e-02,\n","         -1.0192e-01,  4.9506e-02,  1.7848e-02, -4.5306e-02,  9.9468e-02,\n","          5.9525e-02, -9.4046e-02, -3.4359e-02, -9.5282e-02, -1.0241e-01,\n","         -1.0463e-01,  5.2461e-02,  8.7000e-02,  9.2751e-02,  5.3365e-02,\n","          6.1690e-03, -5.8382e-02, -7.7105e-02,  8.2477e-02,  4.3570e-03,\n","         -1.0551e-01, -9.4852e-03, -6.7660e-02, -2.2705e-02,  1.2770e-02,\n","         -1.0440e-01, -9.8047e-03, -1.0163e-01, -1.0392e-01,  9.9941e-02,\n","          8.2711e-02, -2.3932e-02, -5.9301e-02,  4.9658e-02,  9.8632e-02,\n","          9.5108e-02, -1.7580e-02, -3.1445e-02,  6.8888e-02, -1.8463e-04,\n","         -7.6637e-02, -5.6981e-02,  1.4782e-03,  4.1875e-02,  1.1059e-02,\n","         -5.5013e-03, -5.7958e-02, -1.6133e-02, -6.4861e-03, -4.5874e-02,\n","         -7.0499e-02, -4.8100e-02,  1.7645e-02,  4.4669e-02, -5.5278e-02,\n","         -2.5776e-02,  4.1213e-02, -6.5977e-02,  9.4715e-02,  5.8660e-02,\n","         -8.8507e-03,  1.5587e-02, -3.6087e-02,  9.8596e-02,  6.6771e-02,\n","          9.8971e-02,  9.5969e-02, -1.0524e-01,  8.5344e-02,  8.5771e-02,\n","         -7.2028e-02,  1.0793e-01, -7.5006e-02,  6.7214e-02, -3.2775e-02,\n","          8.6797e-02, -8.8291e-02,  4.5558e-02,  5.1202e-03, -8.4349e-02,\n","          8.5221e-02,  1.9687e-02,  4.7816e-02, -6.5804e-02, -7.1686e-02,\n","         -1.6609e-02,  8.5392e-02, -9.0643e-03,  8.1555e-02, -8.1845e-02,\n","         -1.2713e-02, -2.9037e-02, -1.0332e-02, -1.0129e-01, -3.5538e-02,\n","          4.3881e-02, -5.0120e-02,  9.7687e-02, -5.6484e-02,  7.1010e-02,\n","          7.7296e-02,  1.7373e-02, -6.3543e-02, -2.9767e-03, -4.2679e-02,\n","          5.0965e-02,  4.7691e-02,  6.4226e-02, -1.0142e-01, -1.0385e-01,\n","          6.7797e-02, -6.2650e-02,  4.2595e-02, -1.5491e-03,  3.4216e-02,\n","         -3.3556e-02, -1.0012e-01,  9.0341e-02,  4.3065e-02, -1.1963e-02,\n","          4.6413e-02,  9.0636e-02,  4.3935e-02, -8.3152e-02, -4.6955e-02,\n","         -2.6393e-02, -8.0265e-02, -7.7675e-02,  4.1825e-02,  4.6210e-03,\n","         -4.5753e-02, -6.7352e-02, -5.8642e-02, -9.4401e-02,  5.5291e-02,\n","         -1.0009e-01, -1.9324e-02, -2.2546e-02,  7.6260e-02,  6.5295e-02,\n","          2.2131e-02,  4.7731e-02, -7.0337e-02,  5.8093e-02, -1.0801e-01,\n","          3.0280e-02, -9.1425e-02,  4.8900e-02, -6.6315e-02,  1.2800e-02,\n","         -2.6950e-03,  7.8358e-02, -6.0344e-02,  2.0965e-02, -9.5972e-02,\n","         -1.2377e-02, -1.0196e-01, -1.5519e-02,  3.2664e-02,  6.7749e-02,\n","         -2.9113e-02, -7.1227e-02, -2.4140e-02, -1.0792e-01,  7.4076e-02,\n","          4.9067e-02,  6.9297e-02,  5.0764e-02, -9.8634e-02,  7.4107e-02,\n","         -5.7658e-02, -4.8558e-02, -5.9020e-02, -5.7036e-02,  1.0566e-01,\n","          6.5853e-02, -1.0682e-01,  4.0066e-03, -6.6388e-02, -2.3875e-03,\n","          7.9403e-02,  4.4868e-02, -5.6797e-02, -1.9657e-02,  1.3968e-02,\n","         -1.0451e-01,  9.8872e-02,  8.2617e-02, -6.0799e-02, -9.0461e-02,\n","         -5.3770e-02, -1.0189e-01,  2.2081e-02, -1.2834e-02,  1.0374e-01,\n","         -6.3531e-02,  3.6542e-02,  1.0471e-01, -1.8992e-02,  9.1432e-02,\n","          9.8711e-02, -9.2282e-02, -5.7905e-03,  1.0688e-01,  5.8735e-02,\n","         -8.2047e-02,  9.6876e-02,  6.4875e-02,  9.1315e-02,  9.5940e-02,\n","          4.7108e-02, -9.0411e-02, -8.2920e-02,  5.9284e-02,  3.5868e-02,\n","         -1.8582e-03, -3.5123e-02,  8.7533e-02,  1.1188e-02,  3.4626e-02,\n","          7.4988e-02, -3.0644e-02,  9.6146e-03,  2.5563e-05,  8.5461e-02,\n","          5.1745e-02, -2.5779e-02, -4.6519e-03,  5.1806e-03,  2.6616e-02,\n","         -8.4315e-02,  4.1215e-02, -8.2280e-02,  5.3954e-02, -2.9346e-02,\n","          2.7350e-02,  4.7523e-02,  3.6553e-02,  5.5589e-02, -2.1731e-02,\n","          1.0976e-02,  3.1753e-02,  4.2020e-02,  2.2886e-02, -1.1981e-02,\n","         -4.1811e-03,  5.0215e-02,  3.5998e-02, -8.9094e-02,  3.8696e-02,\n","          9.3230e-02,  9.1650e-02, -9.4378e-02,  9.0567e-02,  8.0011e-02,\n","         -1.5092e-02,  6.1519e-02,  2.8040e-02, -7.7474e-02, -8.2591e-02,\n","         -1.0718e-02,  9.3816e-02,  9.8207e-02, -1.0732e-01, -1.4035e-02,\n","          8.3520e-02, -1.0497e-01, -2.0253e-02, -6.1952e-02, -8.2684e-02,\n","         -6.1775e-02,  1.0050e-01, -6.0053e-03, -1.3642e-02, -1.0658e-01,\n","         -9.7789e-02, -1.1713e-02, -6.1080e-02, -3.6314e-02, -8.7158e-02,\n","          5.3183e-02, -1.0309e-01, -3.2178e-03,  7.3087e-02, -2.3036e-02,\n","         -7.9202e-02,  6.3756e-02, -1.0492e-01,  2.5269e-03,  5.3428e-03,\n","         -6.9604e-03, -8.8777e-02,  2.4307e-02, -6.1402e-02,  1.1759e-02,\n","         -9.2349e-02,  1.0180e-01, -2.0717e-02, -8.4975e-02, -9.7469e-02,\n","         -2.4480e-02, -7.7577e-02,  1.3783e-03, -7.6750e-02, -4.2872e-02,\n","          5.9140e-02,  6.7363e-02, -8.4239e-02,  1.0053e-01,  1.1949e-03,\n","         -8.3280e-02, -2.1563e-02]], requires_grad=True)"]},"metadata":{"tags":[]},"execution_count":243}]},{"cell_type":"code","metadata":{"id":"9k08MAGZ326V","colab_type":"code","colab":{}},"source":["# ======================================\n","# relu는 맨 마지막 레이어에서 빼는 것이 좋다.\n","# ======================================\n","\n","#model = torch.nn.Sequential(linear1).to(device)\n","\n","model = torch.nn.Sequential(linear1, relu, dropout, \n","                            linear2, relu, dropout,\n","                            linear3).to(device)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fQxCfqMSces5","colab_type":"text"},"source":["[메뉴얼] https://pytorch.org/docs/stable/nn.html#crossentropyloss"]},{"cell_type":"markdown","metadata":{"id":"UOmqtF8NZ0rN","colab_type":"text"},"source":["## regressor는 손실함수로 MSELoss를 사용한다.\n","\n","- 해당 손실함수로 배추값예측과 같은 선형회귀문제를 풀 수 있다.\n","- 분류문제에서는 softmax를 내부적으로 계산하는 CrossEntropyLoss()를 사용했다."]},{"cell_type":"code","metadata":{"id":"o9WQWlFdcKaH","colab_type":"code","colab":{}},"source":["# 손실함수와 최적화 함수\n","loss = torch.nn.MSELoss().to(device)\n","optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate) "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yQu5p_pTcRFI","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1592217362656,"user_tz":-540,"elapsed":4615,"user":{"displayName":"Juhee","photoUrl":"","userId":"14369088239483395487"}},"outputId":"db372cd5-874a-4e84-cfa8-05d36ae99220"},"source":["total_batch = len(data_loader)\n","for epoch in range(training_epochs):\n","    avg_cost = 0\n","\n","    for X, Y in data_loader:\n","\n","        X = X.to(device)\n","        Y = Y.to(device)\n","\n","        # 그래디언트 초기화\n","        optimizer.zero_grad()\n","        # Forward 계산\n","        hypothesis = model(X)\n","        # Error 계산\n","        cost = loss(hypothesis, Y)\n","        # Backparopagation\n","        cost.backward()\n","        # 가중치 갱신\n","        optimizer.step()\n","\n","        # 평균 Error 계산\n","        avg_cost += cost / total_batch\n","\n","    print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.9f}'.format(avg_cost))\n","\n","print('Learning finished')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([100])) that is different to the input size (torch.Size([100, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n","  return F.mse_loss(input, target, reduction=self.reduction)\n"],"name":"stderr"},{"output_type":"stream","text":["Epoch: 0001 cost = 80362.015625000\n","Epoch: 0002 cost = 757.757751465\n","Epoch: 0003 cost = 636.956909180\n","Epoch: 0004 cost = 624.437255859\n","Epoch: 0005 cost = 639.381958008\n","Epoch: 0006 cost = 626.188354492\n","Epoch: 0007 cost = 641.902832031\n","Epoch: 0008 cost = 614.595520020\n","Epoch: 0009 cost = 617.188842773\n","Epoch: 0010 cost = 625.122253418\n","Epoch: 0011 cost = 602.054565430\n","Epoch: 0012 cost = 585.727539062\n","Epoch: 0013 cost = 590.479187012\n","Epoch: 0014 cost = 607.359497070\n","Epoch: 0015 cost = 599.881103516\n","Epoch: 0016 cost = 576.344482422\n","Epoch: 0017 cost = 582.410217285\n","Epoch: 0018 cost = 580.715332031\n","Epoch: 0019 cost = 588.816772461\n","Epoch: 0020 cost = 582.211608887\n","Epoch: 0021 cost = 572.876220703\n","Epoch: 0022 cost = 550.628417969\n","Epoch: 0023 cost = 547.061645508\n","Epoch: 0024 cost = 540.962890625\n","Epoch: 0025 cost = 560.615417480\n","Epoch: 0026 cost = 550.895629883\n","Epoch: 0027 cost = 553.982788086\n","Epoch: 0028 cost = 533.446960449\n","Epoch: 0029 cost = 539.882141113\n","Epoch: 0030 cost = 514.668945312\n","Epoch: 0031 cost = 515.106933594\n","Epoch: 0032 cost = 527.705688477\n","Epoch: 0033 cost = 508.512115479\n","Epoch: 0034 cost = 514.085449219\n","Epoch: 0035 cost = 517.130065918\n","Epoch: 0036 cost = 493.842376709\n","Epoch: 0037 cost = 478.559173584\n","Epoch: 0038 cost = 479.909759521\n","Epoch: 0039 cost = 471.686126709\n","Epoch: 0040 cost = 469.155792236\n","Epoch: 0041 cost = 478.055480957\n","Epoch: 0042 cost = 469.245117188\n","Epoch: 0043 cost = 454.689086914\n","Epoch: 0044 cost = 453.707305908\n","Epoch: 0045 cost = 435.983703613\n","Epoch: 0046 cost = 437.241088867\n","Epoch: 0047 cost = 426.382293701\n","Epoch: 0048 cost = 432.189788818\n","Epoch: 0049 cost = 430.848968506\n","Epoch: 0050 cost = 424.753143311\n","Epoch: 0051 cost = 409.415039062\n","Epoch: 0052 cost = 423.048858643\n","Epoch: 0053 cost = 403.730224609\n","Epoch: 0054 cost = 398.093078613\n","Epoch: 0055 cost = 380.857971191\n","Epoch: 0056 cost = 404.799316406\n","Epoch: 0057 cost = 388.606719971\n","Epoch: 0058 cost = 381.245849609\n","Epoch: 0059 cost = 374.337402344\n","Epoch: 0060 cost = 376.895538330\n","Epoch: 0061 cost = 358.048675537\n","Epoch: 0062 cost = 368.269256592\n","Epoch: 0063 cost = 355.350250244\n","Epoch: 0064 cost = 354.895690918\n","Epoch: 0065 cost = 359.308410645\n","Epoch: 0066 cost = 346.558654785\n","Epoch: 0067 cost = 351.606384277\n","Epoch: 0068 cost = 341.252746582\n","Epoch: 0069 cost = 331.710296631\n","Epoch: 0070 cost = 331.352172852\n","Epoch: 0071 cost = 328.621887207\n","Epoch: 0072 cost = 313.483520508\n","Epoch: 0073 cost = 305.711669922\n","Epoch: 0074 cost = 311.438934326\n","Epoch: 0075 cost = 319.485900879\n","Epoch: 0076 cost = 296.403045654\n","Epoch: 0077 cost = 313.151885986\n","Epoch: 0078 cost = 304.439544678\n","Epoch: 0079 cost = 299.203796387\n","Epoch: 0080 cost = 289.021789551\n","Epoch: 0081 cost = 287.727508545\n","Epoch: 0082 cost = 267.940399170\n","Epoch: 0083 cost = 273.148437500\n","Epoch: 0084 cost = 290.489166260\n","Epoch: 0085 cost = 275.055419922\n","Epoch: 0086 cost = 261.489013672\n","Epoch: 0087 cost = 258.267333984\n","Epoch: 0088 cost = 260.361694336\n","Epoch: 0089 cost = 249.217208862\n","Epoch: 0090 cost = 263.306579590\n","Epoch: 0091 cost = 261.842254639\n","Epoch: 0092 cost = 249.362762451\n","Epoch: 0093 cost = 253.475830078\n","Epoch: 0094 cost = 235.351913452\n","Epoch: 0095 cost = 240.889999390\n","Epoch: 0096 cost = 229.897735596\n","Epoch: 0097 cost = 223.632080078\n","Epoch: 0098 cost = 229.156158447\n","Epoch: 0099 cost = 231.811126709\n","Epoch: 0100 cost = 229.859619141\n","Epoch: 0101 cost = 224.976104736\n","Epoch: 0102 cost = 216.705276489\n","Epoch: 0103 cost = 230.711730957\n","Epoch: 0104 cost = 232.330245972\n","Epoch: 0105 cost = 220.216567993\n","Epoch: 0106 cost = 211.930694580\n","Epoch: 0107 cost = 209.244094849\n","Epoch: 0108 cost = 217.605331421\n","Epoch: 0109 cost = 206.196899414\n","Epoch: 0110 cost = 187.201873779\n","Epoch: 0111 cost = 186.342559814\n","Epoch: 0112 cost = 193.236083984\n","Epoch: 0113 cost = 198.838531494\n","Epoch: 0114 cost = 190.628936768\n","Epoch: 0115 cost = 199.514938354\n","Epoch: 0116 cost = 188.077758789\n","Epoch: 0117 cost = 194.484222412\n","Epoch: 0118 cost = 180.925140381\n","Epoch: 0119 cost = 182.939361572\n","Epoch: 0120 cost = 177.695907593\n","Epoch: 0121 cost = 165.910537720\n","Epoch: 0122 cost = 171.894195557\n","Epoch: 0123 cost = 180.245910645\n","Epoch: 0124 cost = 175.251800537\n","Epoch: 0125 cost = 169.306533813\n","Epoch: 0126 cost = 170.744354248\n","Epoch: 0127 cost = 168.608642578\n","Epoch: 0128 cost = 153.568191528\n","Epoch: 0129 cost = 159.472427368\n","Epoch: 0130 cost = 150.942840576\n","Epoch: 0131 cost = 157.549362183\n","Epoch: 0132 cost = 167.193344116\n","Epoch: 0133 cost = 166.337432861\n","Epoch: 0134 cost = 151.515838623\n","Epoch: 0135 cost = 141.128707886\n","Epoch: 0136 cost = 139.968902588\n","Epoch: 0137 cost = 154.054199219\n","Epoch: 0138 cost = 129.928314209\n","Epoch: 0139 cost = 144.408569336\n","Epoch: 0140 cost = 139.834426880\n","Epoch: 0141 cost = 148.714965820\n","Epoch: 0142 cost = 142.736297607\n","Epoch: 0143 cost = 132.406890869\n","Epoch: 0144 cost = 143.890563965\n","Epoch: 0145 cost = 144.733551025\n","Epoch: 0146 cost = 127.646713257\n","Epoch: 0147 cost = 139.203308105\n","Epoch: 0148 cost = 142.601669312\n","Epoch: 0149 cost = 137.926757812\n","Epoch: 0150 cost = 134.542358398\n","Epoch: 0151 cost = 133.293273926\n","Epoch: 0152 cost = 129.124328613\n","Epoch: 0153 cost = 128.114440918\n","Epoch: 0154 cost = 128.302658081\n","Epoch: 0155 cost = 123.642929077\n","Epoch: 0156 cost = 124.347549438\n","Epoch: 0157 cost = 127.731002808\n","Epoch: 0158 cost = 130.585601807\n","Epoch: 0159 cost = 118.696304321\n","Epoch: 0160 cost = 124.800666809\n","Epoch: 0161 cost = 121.411911011\n","Epoch: 0162 cost = 130.305984497\n","Epoch: 0163 cost = 125.132888794\n","Epoch: 0164 cost = 117.415710449\n","Epoch: 0165 cost = 110.497840881\n","Epoch: 0166 cost = 124.658508301\n","Epoch: 0167 cost = 116.343666077\n","Epoch: 0168 cost = 116.833709717\n","Epoch: 0169 cost = 123.883071899\n","Epoch: 0170 cost = 117.599456787\n","Epoch: 0171 cost = 115.149742126\n","Epoch: 0172 cost = 117.953178406\n","Epoch: 0173 cost = 118.789352417\n","Epoch: 0174 cost = 117.904502869\n","Epoch: 0175 cost = 110.469436646\n","Epoch: 0176 cost = 116.118194580\n","Epoch: 0177 cost = 114.317916870\n","Epoch: 0178 cost = 111.257843018\n","Epoch: 0179 cost = 109.141174316\n","Epoch: 0180 cost = 108.325775146\n","Epoch: 0181 cost = 108.740257263\n","Epoch: 0182 cost = 101.005966187\n","Epoch: 0183 cost = 104.254478455\n","Epoch: 0184 cost = 114.245605469\n","Epoch: 0185 cost = 109.356430054\n","Epoch: 0186 cost = 100.382858276\n","Epoch: 0187 cost = 96.777702332\n","Epoch: 0188 cost = 100.510513306\n","Epoch: 0189 cost = 97.550209045\n","Epoch: 0190 cost = 111.049766541\n","Epoch: 0191 cost = 105.964904785\n","Epoch: 0192 cost = 103.329605103\n","Epoch: 0193 cost = 103.848114014\n","Epoch: 0194 cost = 96.981079102\n","Epoch: 0195 cost = 101.980758667\n","Epoch: 0196 cost = 96.650009155\n","Epoch: 0197 cost = 96.329315186\n","Epoch: 0198 cost = 99.466178894\n","Epoch: 0199 cost = 105.393455505\n","Epoch: 0200 cost = 98.890396118\n","Epoch: 0201 cost = 100.718849182\n","Epoch: 0202 cost = 100.236610413\n","Epoch: 0203 cost = 102.039726257\n","Epoch: 0204 cost = 96.597564697\n","Epoch: 0205 cost = 99.979400635\n","Epoch: 0206 cost = 99.573631287\n","Epoch: 0207 cost = 94.890838623\n","Epoch: 0208 cost = 96.938949585\n","Epoch: 0209 cost = 96.938583374\n","Epoch: 0210 cost = 90.781845093\n","Epoch: 0211 cost = 93.319458008\n","Epoch: 0212 cost = 100.127365112\n","Epoch: 0213 cost = 101.186828613\n","Epoch: 0214 cost = 90.589019775\n","Epoch: 0215 cost = 99.621780396\n","Epoch: 0216 cost = 95.875709534\n","Epoch: 0217 cost = 99.414016724\n","Epoch: 0218 cost = 95.813568115\n","Epoch: 0219 cost = 90.514381409\n","Epoch: 0220 cost = 96.433784485\n","Epoch: 0221 cost = 98.178001404\n","Epoch: 0222 cost = 96.769638062\n","Epoch: 0223 cost = 91.675552368\n","Epoch: 0224 cost = 86.795883179\n","Epoch: 0225 cost = 91.462860107\n","Epoch: 0226 cost = 92.470169067\n","Epoch: 0227 cost = 88.776306152\n","Epoch: 0228 cost = 88.742462158\n","Epoch: 0229 cost = 94.402221680\n","Epoch: 0230 cost = 96.602828979\n","Epoch: 0231 cost = 96.708572388\n","Epoch: 0232 cost = 84.543838501\n","Epoch: 0233 cost = 88.908294678\n","Epoch: 0234 cost = 91.697479248\n","Epoch: 0235 cost = 84.214721680\n","Epoch: 0236 cost = 87.574775696\n","Epoch: 0237 cost = 86.132682800\n","Epoch: 0238 cost = 92.107894897\n","Epoch: 0239 cost = 90.102294922\n","Epoch: 0240 cost = 92.073043823\n","Epoch: 0241 cost = 89.452331543\n","Epoch: 0242 cost = 90.811538696\n","Epoch: 0243 cost = 90.509971619\n","Epoch: 0244 cost = 89.985816956\n","Epoch: 0245 cost = 90.756736755\n","Epoch: 0246 cost = 89.717086792\n","Epoch: 0247 cost = 83.856292725\n","Epoch: 0248 cost = 89.852783203\n","Epoch: 0249 cost = 88.048782349\n","Epoch: 0250 cost = 91.240737915\n","Epoch: 0251 cost = 93.827346802\n","Epoch: 0252 cost = 92.345252991\n","Epoch: 0253 cost = 92.033042908\n","Epoch: 0254 cost = 94.618377686\n","Epoch: 0255 cost = 94.559173584\n","Epoch: 0256 cost = 93.353172302\n","Epoch: 0257 cost = 91.576148987\n","Epoch: 0258 cost = 91.387115479\n","Epoch: 0259 cost = 90.847595215\n","Epoch: 0260 cost = 89.109336853\n","Epoch: 0261 cost = 91.802627563\n","Epoch: 0262 cost = 88.119873047\n","Epoch: 0263 cost = 96.453231812\n","Epoch: 0264 cost = 88.599586487\n","Epoch: 0265 cost = 89.505096436\n","Epoch: 0266 cost = 94.026367188\n","Epoch: 0267 cost = 88.245826721\n","Epoch: 0268 cost = 95.176651001\n","Epoch: 0269 cost = 88.399742126\n","Epoch: 0270 cost = 90.182144165\n","Epoch: 0271 cost = 93.343704224\n","Epoch: 0272 cost = 93.018302917\n","Epoch: 0273 cost = 86.437461853\n","Epoch: 0274 cost = 83.743797302\n","Epoch: 0275 cost = 88.087203979\n","Epoch: 0276 cost = 85.948913574\n","Epoch: 0277 cost = 90.069564819\n","Epoch: 0278 cost = 93.351272583\n","Epoch: 0279 cost = 95.376091003\n","Epoch: 0280 cost = 89.560417175\n","Epoch: 0281 cost = 95.260643005\n","Epoch: 0282 cost = 93.614944458\n","Epoch: 0283 cost = 85.640342712\n","Epoch: 0284 cost = 89.751007080\n","Epoch: 0285 cost = 89.880264282\n","Epoch: 0286 cost = 89.273811340\n","Epoch: 0287 cost = 93.296714783\n","Epoch: 0288 cost = 80.636482239\n","Epoch: 0289 cost = 83.365501404\n","Epoch: 0290 cost = 89.833839417\n","Epoch: 0291 cost = 92.278366089\n","Epoch: 0292 cost = 84.086967468\n","Epoch: 0293 cost = 90.536926270\n","Epoch: 0294 cost = 94.003425598\n","Epoch: 0295 cost = 90.687362671\n","Epoch: 0296 cost = 90.096115112\n","Epoch: 0297 cost = 88.170806885\n","Epoch: 0298 cost = 91.244323730\n","Epoch: 0299 cost = 83.626113892\n","Epoch: 0300 cost = 90.261169434\n","Learning finished\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"AHccupIjbm4g","colab_type":"text"},"source":["\n","\n","### test 셋도 Scaler를 통해 transofrom을 해준다.\n","\n","\n","\n","1.   train 셋의 transfrom:   **fit을 사용**\n","- Scaler.fit_transform(x_train_data)\n","- fit -> train 데이터셋을 통해서 어떤 방식으로 scaler를 할지 계산을 구한다는 의미.\n","\n","2. test셋의 transform: **그냥 transform만**\n","-  Scaler.transform(x_test_data) \n","- fit_transform(train)을 통해 구해진 scaler 방식을 test셋에 적용하겠다는 의미. "]},{"cell_type":"code","metadata":{"id":"z_SQp0GxdJfc","colab_type":"code","colab":{}},"source":["# Test the model using test sets\n","with torch.no_grad():\n","\n","  x_test_data=test_data.loc[:,:]\n","  x_test_data=np.array(x_test_data)\n","  x_test_data = Scaler.transform(x_test_data)         # test 셋도 Scaler를 통해 transofrom을 해준다.      \n","  x_test_data=torch.from_numpy(x_test_data).float().to(device)\n","\n","  prediction = model(x_test_data)\n","\n","# argmax를 하지 않는다. predication자체가 예측값이 되기 때문에."],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mZK0F3SY0m8y","colab_type":"code","colab":{}},"source":["correct_prediction = prediction.cpu().numpy().reshape(-1,1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qz8oLaz50or_","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":399},"executionInfo":{"status":"ok","timestamp":1592217363100,"user_tz":-540,"elapsed":4959,"user":{"displayName":"Juhee","photoUrl":"","userId":"14369088239483395487"}},"outputId":"820d6331-27c7-490a-ce32-792a4947f65d"},"source":["submit=pd.read_csv('sample_submission.csv')\n","submit"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>Price</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2.0</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>3.0</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>4.0</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>162</th>\n","      <td>162.0</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th>163</th>\n","      <td>163.0</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th>164</th>\n","      <td>164.0</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th>165</th>\n","      <td>165.0</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th>166</th>\n","      <td>166.0</td>\n","      <td>1.0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>167 rows × 2 columns</p>\n","</div>"],"text/plain":["        id  Price\n","0      0.0    1.0\n","1      1.0    1.0\n","2      2.0    1.0\n","3      3.0    1.0\n","4      4.0    1.0\n","..     ...    ...\n","162  162.0    1.0\n","163  163.0    1.0\n","164  164.0    1.0\n","165  165.0    1.0\n","166  166.0    1.0\n","\n","[167 rows x 2 columns]"]},"metadata":{"tags":[]},"execution_count":249}]},{"cell_type":"code","metadata":{"id":"REN-barMGaQg","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":399},"executionInfo":{"status":"ok","timestamp":1592217363102,"user_tz":-540,"elapsed":4927,"user":{"displayName":"Juhee","photoUrl":"","userId":"14369088239483395487"}},"outputId":"fcf2548e-d709-41e2-910b-3a8e9cf99b73"},"source":["for i in range(len(correct_prediction)):\n","  submit['Price'][i]=correct_prediction[i].item()\n","\n","submit"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>Price</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0.0</td>\n","      <td>22.429939</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1.0</td>\n","      <td>22.429939</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2.0</td>\n","      <td>22.429939</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>3.0</td>\n","      <td>22.429939</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>4.0</td>\n","      <td>22.429939</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>162</th>\n","      <td>162.0</td>\n","      <td>22.429939</td>\n","    </tr>\n","    <tr>\n","      <th>163</th>\n","      <td>163.0</td>\n","      <td>22.429939</td>\n","    </tr>\n","    <tr>\n","      <th>164</th>\n","      <td>164.0</td>\n","      <td>22.429939</td>\n","    </tr>\n","    <tr>\n","      <th>165</th>\n","      <td>165.0</td>\n","      <td>22.429939</td>\n","    </tr>\n","    <tr>\n","      <th>166</th>\n","      <td>166.0</td>\n","      <td>22.429939</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>167 rows × 2 columns</p>\n","</div>"],"text/plain":["        id      Price\n","0      0.0  22.429939\n","1      1.0  22.429939\n","2      2.0  22.429939\n","3      3.0  22.429939\n","4      4.0  22.429939\n","..     ...        ...\n","162  162.0  22.429939\n","163  163.0  22.429939\n","164  164.0  22.429939\n","165  165.0  22.429939\n","166  166.0  22.429939\n","\n","[167 rows x 2 columns]"]},"metadata":{"tags":[]},"execution_count":250}]},{"cell_type":"code","metadata":{"id":"SKu0HYd70tNp","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":52},"executionInfo":{"status":"ok","timestamp":1592217377385,"user_tz":-540,"elapsed":19178,"user":{"displayName":"Juhee","photoUrl":"","userId":"14369088239483395487"}},"outputId":"433d47e2-d297-43c1-f90b-ce4b0af519bb"},"source":["submit.to_csv('baseline.csv',index=False,header=True)\n","\n","!kaggle competitions submit -c 2020aiboston -f baseline.csv -m \"3layer dropout, 13>512>1\""],"execution_count":null,"outputs":[{"output_type":"stream","text":["100% 3.82k/3.82k [00:10<00:00, 377B/s]\n","Successfully submitted to 2020.AI.Boston"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"lc4m7_pLbxG_","colab_type":"text"},"source":["### Evaluation: RMSE\n","- 해당 regression문제의 Evaluation은 RMSE 이다. (분류문제는 accuracy를 사용)\n","- 0에 가까운 값이 이상적인 값(답)이다."]},{"cell_type":"code","metadata":{"id":"3ru-4NG-Gouz","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]}]}