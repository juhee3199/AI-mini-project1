{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Boston_dnn_xavier_kaggle.ipynb","provenance":[{"file_id":"1n7Az-_b6wF7hDOh6p2CG7EKzsQRE4xhz","timestamp":1592214114251},{"file_id":"1y9qF3D4vbQVhX_dZCgplRuw_EcgW8Sg2","timestamp":1591114496797},{"file_id":"1CI11CA5otqB7-RsQLKak3LJW3xeCUr9q","timestamp":1590895708437},{"file_id":"1jsP6IkoSZW5roMip53QSPJjiVP6Ej3Zk","timestamp":1590895018573},{"file_id":"1bimNZtout48XzSy7VoWbmdpOHQ-SAkHX","timestamp":1590864591861}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"d3NuyPLviZzA","colab_type":"text"},"source":["### boston 집값 예측\n","target value: 실제 주택 가격"]},{"cell_type":"markdown","metadata":{"id":"jE4dHYRtXLOo","colab_type":"text"},"source":["### NN regressor 모델 설계\n","- classification과 크게 코드차이가 없다.\n","- 차이:\n","1.   train셋과 test 셋 Scaler를 통해 transofrom을 해준다.\n","- 데이터의 분포가 다 다르기때문에 scaler를 맞춰주었다. preprocessing.StandardScaler() 를 사용.\n","2.   손실함수로 MSELoss를 사용한다.\n","3. 평가는 RMSE \n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"m4bGbvfFh-VV","colab_type":"text"},"source":["## baseline을 넘기 위해 내가 한 전략\n","- 1) 1layer 에서 3layer사용 -> 성능이 더 좋지 않음.\n","- 2) 3lyaer + dropout 적용-> **baseline을 넘겼다.**\n","- 3) 2번에서 13>256>1 을 13>512>1 로 변경했다. -> 성능이 조금 더 향상되었다. "]},{"cell_type":"code","metadata":{"id":"1ufl-SW8BorT","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":339},"executionInfo":{"status":"ok","timestamp":1592214709540,"user_tz":-540,"elapsed":12535,"user":{"displayName":"Juhee","photoUrl":"","userId":"14369088239483395487"}},"outputId":"27895f31-50f1-4469-cf03-2f10fab1a3bd"},"source":["!pip uninstall -y kaggle\n","!pip install --upgrade pip\n","!pip install kaggle==1.5.6"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Found existing installation: kaggle 1.5.6\n","Uninstalling kaggle-1.5.6:\n","  Successfully uninstalled kaggle-1.5.6\n","Requirement already up-to-date: pip in /usr/local/lib/python3.6/dist-packages (20.1.1)\n","Processing /root/.cache/pip/wheels/01/3e/ff/77407ebac3ef71a79b9166a8382aecf88415a0bcbe3c095a01/kaggle-1.5.6-py3-none-any.whl\n","Requirement already satisfied: certifi in /usr/local/lib/python3.6/dist-packages (from kaggle==1.5.6) (2020.4.5.1)\n","Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from kaggle==1.5.6) (1.24.3)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from kaggle==1.5.6) (4.41.1)\n","Requirement already satisfied: python-dateutil in /usr/local/lib/python3.6/dist-packages (from kaggle==1.5.6) (2.8.1)\n","Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from kaggle==1.5.6) (2.23.0)\n","Requirement already satisfied: python-slugify in /usr/local/lib/python3.6/dist-packages (from kaggle==1.5.6) (4.0.0)\n","Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.6/dist-packages (from kaggle==1.5.6) (1.12.0)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->kaggle==1.5.6) (2.9)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->kaggle==1.5.6) (3.0.4)\n","Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.6/dist-packages (from python-slugify->kaggle==1.5.6) (1.3)\n","Installing collected packages: kaggle\n","Successfully installed kaggle-1.5.6\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"IC8Uz2G9B64g","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1592214716589,"user_tz":-540,"elapsed":19527,"user":{"displayName":"Juhee","photoUrl":"","userId":"14369088239483395487"}},"outputId":"3a61b215-f0f7-4ec3-c319-c98cee482208"},"source":["!mkdir -p ~/.kaggle\n","!cp kaggle.json ~/.kaggle\n","!ls -lha kaggle.json\n","!chmod 600 ~/.kaggle/kaggle.json"],"execution_count":null,"outputs":[{"output_type":"stream","text":["-rw-r--r-- 1 root root 64 Jun 15 09:42 kaggle.json\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"puERE7IqCBNs","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":70},"executionInfo":{"status":"ok","timestamp":1592214720217,"user_tz":-540,"elapsed":23107,"user":{"displayName":"Juhee","photoUrl":"","userId":"14369088239483395487"}},"outputId":"36dd41ba-4ee7-4b9c-dec5-0fcff5bd46ad"},"source":["!kaggle competitions download -c 2020aiboston"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Downloading 2020aiboston.zip to /content\n","\r  0% 0.00/14.5k [00:00<?, ?B/s]\n","\r100% 14.5k/14.5k [00:00<00:00, 11.9MB/s]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Jf2XBDZlCEt4","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":88},"executionInfo":{"status":"ok","timestamp":1592214723038,"user_tz":-540,"elapsed":25758,"user":{"displayName":"Juhee","photoUrl":"","userId":"14369088239483395487"}},"outputId":"8230e2ed-bd87-428a-b9d1-7329be25d408"},"source":["!unzip 2020aiboston.zip"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Archive:  2020aiboston.zip\n","  inflating: boston_housing_test.csv  \n","  inflating: boston_housing_train.csv  \n","  inflating: sample_submission.csv   \n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"9EVz_27QV_AJ","colab_type":"code","colab":{}},"source":["train_data=pd.read_csv('boston_housing_train.csv',header=None, usecols=range(1,15))\n","test_data=pd.read_csv('boston_housing_test.csv',header=None, usecols=range(1,14))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Y3bpvTNIRwWl","colab_type":"code","colab":{}},"source":["import pandas as pd\n","import numpy as np\n","\n","import torch\n","import torchvision.datasets as data\n","import torchvision.transforms as transforms\n","import random\n","\n","from sklearn import preprocessing"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QICWFIJUR6PL","colab_type":"code","colab":{}},"source":["device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","\n","random.seed(777)\n","torch.manual_seed(777)\n","if device == 'cuda':\n","  torch.cuda.manual_seed_all(777)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gh_Yz21HVs0V","colab_type":"code","colab":{}},"source":["# 학습 파라미터 설정\n","learning_rate = 0.1\n","training_epochs = 300\n","batch_size = 100\n","\n","drop_prob = 0.3\n","\n","Scaler = preprocessing.StandardScaler()  \n","\n","# 데이터의 분포가 다 다르다. 이를 preprocessing.StandardScaler()를 이용해  Scaler를 맞춰주었다. "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"w94IlzOdC_m2","colab_type":"code","colab":{}},"source":["x_train_data=train_data.loc[:,0:13]\n","y_train_data=train_data.loc[:,14]\n","\n","x_train_data=np.array(x_train_data)\n","y_train_data=np.array(y_train_data)\n","x_train_data = Scaler.fit_transform(x_train_data)\n","\n","x_train_data=torch.FloatTensor(x_train_data)\n","y_train_data=torch.FloatTensor(y_train_data)      # y데이터가 FloatTensor 형이다."],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wBz_3lepEHYX","colab_type":"code","colab":{}},"source":["train_dataset = torch.utils.data.TensorDataset(x_train_data, y_train_data)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-Ae1Q7WXWJcc","colab_type":"code","colab":{}},"source":["# Tensor 데이터셋을 통해 DataLoader를 사용할 수 있다.\n","\n","data_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n","                                          batch_size=batch_size,\n","                                          shuffle=True,\n","                                          drop_last=True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"R0Tk0BBFzOJq","colab_type":"text"},"source":["![대체 텍스트](https://user-images.githubusercontent.com/11758940/83336289-229ec200-a2ed-11ea-9fb8-88a51198e475.png)"]},{"cell_type":"code","metadata":{"id":"ntcrgdljaxST","colab_type":"code","colab":{}},"source":["linear1 = torch.nn.Linear(13,256,bias=True)\n","linear2 = torch.nn.Linear(256,256,bias=True)\n","linear3 = torch.nn.Linear(256,256,bias=True)\n","linear4 = torch.nn.Linear(256,1,bias=True)\n","\n","relu = torch.nn.ReLU()\n","dropout = torch.nn.Dropout(p=drop_prob)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OUqE8Uibc4xH","colab_type":"text"},"source":["[메뉴얼] https://pytorch.org/docs/stable/nn.init.html#torch.nn.init.xavier_uniform_"]},{"cell_type":"code","metadata":{"id":"MrpNg6XycEWE","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":621},"executionInfo":{"status":"ok","timestamp":1592217622293,"user_tz":-540,"elapsed":1544,"user":{"displayName":"Juhee","photoUrl":"","userId":"14369088239483395487"}},"outputId":"7dac890f-2b5e-44ac-cb24-612a052d94a0"},"source":["# Random Init => Xavier Init\n","torch.nn.init.xavier_uniform_(linear1.weight)\n","torch.nn.init.xavier_uniform_(linear2.weight)\n","torch.nn.init.xavier_uniform_(linear3.weight)\n","torch.nn.init.xavier_uniform_(linear4.weight)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Parameter containing:\n","tensor([[ 0.0356,  0.0085,  0.0999,  0.0127,  0.1249, -0.0273, -0.0803, -0.0915,\n","          0.0858, -0.0625,  0.1165,  0.1332, -0.0778, -0.1137,  0.0287,  0.0344,\n","          0.1482, -0.0813, -0.0298,  0.1211, -0.1243,  0.1513, -0.1211,  0.0975,\n","          0.0011, -0.0973, -0.0462, -0.0931,  0.0724,  0.0377,  0.1494, -0.0279,\n","          0.0648,  0.1255,  0.1171, -0.0611,  0.0891,  0.1256, -0.0317,  0.0219,\n","         -0.0679,  0.0643, -0.0786,  0.1386,  0.1097,  0.0322,  0.0389,  0.1511,\n","         -0.0558,  0.0662,  0.1030,  0.0803, -0.0410,  0.0009,  0.0152, -0.0615,\n","          0.1490,  0.1371, -0.0273, -0.1022, -0.0032, -0.0722, -0.1426,  0.0484,\n","          0.1214, -0.1152,  0.0584, -0.0672, -0.0806, -0.0023, -0.1467, -0.1266,\n","         -0.1166, -0.1104,  0.0042,  0.1296,  0.0402, -0.1188,  0.0973, -0.0215,\n","         -0.0950, -0.0459, -0.1372,  0.1183,  0.0629, -0.1350,  0.1478, -0.0270,\n","          0.1293, -0.0895,  0.0013, -0.0792,  0.0190,  0.0782,  0.1524, -0.1087,\n","          0.1335, -0.1126,  0.0359,  0.0632,  0.0244, -0.0979, -0.1204, -0.0754,\n","         -0.0907, -0.1162, -0.0368,  0.1311, -0.0507, -0.1401,  0.0793, -0.0692,\n","          0.1063,  0.0471, -0.1457,  0.0251,  0.0398,  0.0447,  0.1202,  0.0372,\n","         -0.0933,  0.0757,  0.0026, -0.1185, -0.0156, -0.1427, -0.0350, -0.0236,\n","          0.0468,  0.1353, -0.1260,  0.1510,  0.1218,  0.1387,  0.1519,  0.1344,\n","         -0.1360, -0.0433,  0.0343,  0.1358,  0.1457, -0.0560, -0.1078, -0.0582,\n","          0.0980, -0.0254, -0.0132,  0.0227, -0.0837, -0.0422,  0.0741,  0.0830,\n","          0.1316,  0.0350,  0.1256, -0.1254, -0.1449, -0.1160,  0.0659, -0.1291,\n","         -0.0667, -0.0297,  0.1342, -0.1145,  0.0083,  0.0799, -0.1392, -0.1309,\n","         -0.0562, -0.1148, -0.0224, -0.0923,  0.1303,  0.0274, -0.0756, -0.0050,\n","         -0.0297,  0.1363,  0.0348,  0.1139, -0.0220, -0.0443, -0.1367, -0.1413,\n","          0.0239, -0.0135,  0.1202,  0.1124,  0.0910, -0.1099,  0.1364,  0.0806,\n","          0.0515,  0.0212, -0.0990, -0.0932, -0.1336, -0.1072, -0.0076, -0.0899,\n","          0.1504,  0.1137, -0.1092, -0.0201, -0.0769,  0.0699,  0.1323,  0.1403,\n","          0.1177, -0.1097,  0.0840,  0.1049,  0.0522,  0.0117,  0.1509, -0.0100,\n","          0.0070,  0.0631,  0.0809, -0.1480, -0.1315,  0.0804, -0.0754,  0.1487,\n","          0.0903, -0.0071,  0.0190,  0.0192,  0.0817,  0.0130,  0.1357,  0.0251,\n","         -0.1437, -0.0948, -0.0844,  0.0033,  0.0495,  0.0859,  0.0306, -0.0205,\n","         -0.0927,  0.0225, -0.0926,  0.1047, -0.0296,  0.1276,  0.1459, -0.1477,\n","         -0.0759, -0.0873,  0.0838, -0.0271, -0.1014,  0.1455, -0.0803, -0.0202]],\n","       requires_grad=True)"]},"metadata":{"tags":[]},"execution_count":273}]},{"cell_type":"code","metadata":{"id":"9k08MAGZ326V","colab_type":"code","colab":{}},"source":["# ======================================\n","# relu는 맨 마지막 레이어에서 빼는 것이 좋다.\n","# ======================================\n","\n","#model = torch.nn.Sequential(linear1).to(device)\n","\n","model = torch.nn.Sequential(linear1, relu, dropout, \n","                            linear2, relu, dropout,\n","                            linear3, relu, dropout,\n","                            linear4).to(device)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fQxCfqMSces5","colab_type":"text"},"source":["[메뉴얼] https://pytorch.org/docs/stable/nn.html#crossentropyloss"]},{"cell_type":"markdown","metadata":{"id":"UOmqtF8NZ0rN","colab_type":"text"},"source":["## regressor는 손실함수로 MSELoss를 사용한다.\n","\n","- 해당 손실함수로 배추값예측과 같은 선형회귀문제를 풀 수 있다.\n","- 분류문제에서는 softmax를 내부적으로 계산하는 CrossEntropyLoss()를 사용했다."]},{"cell_type":"code","metadata":{"id":"o9WQWlFdcKaH","colab_type":"code","colab":{}},"source":["# 손실함수와 최적화 함수\n","loss = torch.nn.MSELoss().to(device)\n","optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate) "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yQu5p_pTcRFI","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1592217626455,"user_tz":-540,"elapsed":5597,"user":{"displayName":"Juhee","photoUrl":"","userId":"14369088239483395487"}},"outputId":"e500e2ab-d412-462b-e731-8b8243b842c1"},"source":["total_batch = len(data_loader)\n","for epoch in range(training_epochs):\n","    avg_cost = 0\n","\n","    for X, Y in data_loader:\n","\n","        X = X.to(device)\n","        Y = Y.to(device)\n","\n","        # 그래디언트 초기화\n","        optimizer.zero_grad()\n","        # Forward 계산\n","        hypothesis = model(X)\n","        # Error 계산\n","        cost = loss(hypothesis, Y)\n","        # Backparopagation\n","        cost.backward()\n","        # 가중치 갱신\n","        optimizer.step()\n","\n","        # 평균 Error 계산\n","        avg_cost += cost / total_batch\n","\n","    print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.9f}'.format(avg_cost))\n","\n","print('Learning finished')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([100])) that is different to the input size (torch.Size([100, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n","  return F.mse_loss(input, target, reduction=self.reduction)\n"],"name":"stderr"},{"output_type":"stream","text":["Epoch: 0001 cost = 1579533.625000000\n","Epoch: 0002 cost = 793.483093262\n","Epoch: 0003 cost = 489.312561035\n","Epoch: 0004 cost = 634.474365234\n","Epoch: 0005 cost = 644.295104980\n","Epoch: 0006 cost = 623.824157715\n","Epoch: 0007 cost = 641.734008789\n","Epoch: 0008 cost = 543.231750488\n","Epoch: 0009 cost = 500.645507812\n","Epoch: 0010 cost = 447.541015625\n","Epoch: 0011 cost = 468.923431396\n","Epoch: 0012 cost = 410.712707520\n","Epoch: 0013 cost = 369.593841553\n","Epoch: 0014 cost = 342.590057373\n","Epoch: 0015 cost = 302.263763428\n","Epoch: 0016 cost = 296.847106934\n","Epoch: 0017 cost = 254.333984375\n","Epoch: 0018 cost = 231.493316650\n","Epoch: 0019 cost = 212.317230225\n","Epoch: 0020 cost = 182.187927246\n","Epoch: 0021 cost = 191.683166504\n","Epoch: 0022 cost = 195.995635986\n","Epoch: 0023 cost = 188.741271973\n","Epoch: 0024 cost = 191.706817627\n","Epoch: 0025 cost = 202.914581299\n","Epoch: 0026 cost = 188.784255981\n","Epoch: 0027 cost = 182.536666870\n","Epoch: 0028 cost = 160.969024658\n","Epoch: 0029 cost = 191.067718506\n","Epoch: 0030 cost = 182.472671509\n","Epoch: 0031 cost = 182.681777954\n","Epoch: 0032 cost = 185.142181396\n","Epoch: 0033 cost = 182.847869873\n","Epoch: 0034 cost = 188.459899902\n","Epoch: 0035 cost = 190.715560913\n","Epoch: 0036 cost = 175.667419434\n","Epoch: 0037 cost = 179.727264404\n","Epoch: 0038 cost = 180.975738525\n","Epoch: 0039 cost = 180.788558960\n","Epoch: 0040 cost = 196.001770020\n","Epoch: 0041 cost = 168.348266602\n","Epoch: 0042 cost = 181.827239990\n","Epoch: 0043 cost = 191.778686523\n","Epoch: 0044 cost = 179.560455322\n","Epoch: 0045 cost = 180.174987793\n","Epoch: 0046 cost = 175.365219116\n","Epoch: 0047 cost = 177.941955566\n","Epoch: 0048 cost = 182.267791748\n","Epoch: 0049 cost = 172.229309082\n","Epoch: 0050 cost = 178.161758423\n","Epoch: 0051 cost = 188.289703369\n","Epoch: 0052 cost = 186.733001709\n","Epoch: 0053 cost = 167.708862305\n","Epoch: 0054 cost = 185.149261475\n","Epoch: 0055 cost = 185.704986572\n","Epoch: 0056 cost = 192.140594482\n","Epoch: 0057 cost = 164.096603394\n","Epoch: 0058 cost = 162.047561646\n","Epoch: 0059 cost = 183.113891602\n","Epoch: 0060 cost = 169.715454102\n","Epoch: 0061 cost = 185.826873779\n","Epoch: 0062 cost = 187.319107056\n","Epoch: 0063 cost = 199.260437012\n","Epoch: 0064 cost = 184.029144287\n","Epoch: 0065 cost = 179.044952393\n","Epoch: 0066 cost = 195.434799194\n","Epoch: 0067 cost = 188.909484863\n","Epoch: 0068 cost = 183.263061523\n","Epoch: 0069 cost = 175.534530640\n","Epoch: 0070 cost = 180.717300415\n","Epoch: 0071 cost = 170.704101562\n","Epoch: 0072 cost = 195.689514160\n","Epoch: 0073 cost = 173.417999268\n","Epoch: 0074 cost = 161.987472534\n","Epoch: 0075 cost = 179.462371826\n","Epoch: 0076 cost = 193.740264893\n","Epoch: 0077 cost = 190.451171875\n","Epoch: 0078 cost = 174.110870361\n","Epoch: 0079 cost = 196.649291992\n","Epoch: 0080 cost = 185.311218262\n","Epoch: 0081 cost = 173.366958618\n","Epoch: 0082 cost = 185.836059570\n","Epoch: 0083 cost = 182.923461914\n","Epoch: 0084 cost = 179.807449341\n","Epoch: 0085 cost = 199.596267700\n","Epoch: 0086 cost = 172.101821899\n","Epoch: 0087 cost = 174.891662598\n","Epoch: 0088 cost = 198.441619873\n","Epoch: 0089 cost = 157.410095215\n","Epoch: 0090 cost = 173.902069092\n","Epoch: 0091 cost = 182.197387695\n","Epoch: 0092 cost = 189.901077271\n","Epoch: 0093 cost = 164.129409790\n","Epoch: 0094 cost = 173.036895752\n","Epoch: 0095 cost = 189.714660645\n","Epoch: 0096 cost = 167.682327271\n","Epoch: 0097 cost = 185.046356201\n","Epoch: 0098 cost = 185.017669678\n","Epoch: 0099 cost = 175.778457642\n","Epoch: 0100 cost = 176.104232788\n","Epoch: 0101 cost = 196.684677124\n","Epoch: 0102 cost = 174.769332886\n","Epoch: 0103 cost = 167.745254517\n","Epoch: 0104 cost = 174.464355469\n","Epoch: 0105 cost = 172.933898926\n","Epoch: 0106 cost = 172.806457520\n","Epoch: 0107 cost = 176.674453735\n","Epoch: 0108 cost = 184.812881470\n","Epoch: 0109 cost = 181.898437500\n","Epoch: 0110 cost = 169.334289551\n","Epoch: 0111 cost = 189.093780518\n","Epoch: 0112 cost = 175.309783936\n","Epoch: 0113 cost = 174.770935059\n","Epoch: 0114 cost = 186.787475586\n","Epoch: 0115 cost = 172.921997070\n","Epoch: 0116 cost = 187.601531982\n","Epoch: 0117 cost = 179.569732666\n","Epoch: 0118 cost = 175.888793945\n","Epoch: 0119 cost = 179.536346436\n","Epoch: 0120 cost = 178.999313354\n","Epoch: 0121 cost = 159.305480957\n","Epoch: 0122 cost = 177.546966553\n","Epoch: 0123 cost = 170.485519409\n","Epoch: 0124 cost = 196.313903809\n","Epoch: 0125 cost = 160.651245117\n","Epoch: 0126 cost = 171.658325195\n","Epoch: 0127 cost = 182.202438354\n","Epoch: 0128 cost = 170.574707031\n","Epoch: 0129 cost = 175.916900635\n","Epoch: 0130 cost = 170.677398682\n","Epoch: 0131 cost = 176.642913818\n","Epoch: 0132 cost = 173.774871826\n","Epoch: 0133 cost = 175.403656006\n","Epoch: 0134 cost = 181.036865234\n","Epoch: 0135 cost = 186.055511475\n","Epoch: 0136 cost = 164.634292603\n","Epoch: 0137 cost = 166.558395386\n","Epoch: 0138 cost = 188.076492310\n","Epoch: 0139 cost = 171.342697144\n","Epoch: 0140 cost = 177.300628662\n","Epoch: 0141 cost = 169.439865112\n","Epoch: 0142 cost = 181.472503662\n","Epoch: 0143 cost = 171.866348267\n","Epoch: 0144 cost = 177.305938721\n","Epoch: 0145 cost = 181.210891724\n","Epoch: 0146 cost = 170.283569336\n","Epoch: 0147 cost = 170.649490356\n","Epoch: 0148 cost = 172.404281616\n","Epoch: 0149 cost = 179.440124512\n","Epoch: 0150 cost = 163.890197754\n","Epoch: 0151 cost = 162.102935791\n","Epoch: 0152 cost = 173.081207275\n","Epoch: 0153 cost = 179.676101685\n","Epoch: 0154 cost = 166.411560059\n","Epoch: 0155 cost = 175.493255615\n","Epoch: 0156 cost = 170.878402710\n","Epoch: 0157 cost = 167.740814209\n","Epoch: 0158 cost = 177.773574829\n","Epoch: 0159 cost = 158.126220703\n","Epoch: 0160 cost = 159.550125122\n","Epoch: 0161 cost = 171.937026978\n","Epoch: 0162 cost = 164.327713013\n","Epoch: 0163 cost = 174.776733398\n","Epoch: 0164 cost = 170.680084229\n","Epoch: 0165 cost = 169.323699951\n","Epoch: 0166 cost = 170.302093506\n","Epoch: 0167 cost = 165.948730469\n","Epoch: 0168 cost = 168.247985840\n","Epoch: 0169 cost = 162.799957275\n","Epoch: 0170 cost = 165.480926514\n","Epoch: 0171 cost = 163.919052124\n","Epoch: 0172 cost = 161.255203247\n","Epoch: 0173 cost = 160.707794189\n","Epoch: 0174 cost = 175.981140137\n","Epoch: 0175 cost = 176.687667847\n","Epoch: 0176 cost = 151.296264648\n","Epoch: 0177 cost = 177.735626221\n","Epoch: 0178 cost = 168.570755005\n","Epoch: 0179 cost = 155.814453125\n","Epoch: 0180 cost = 161.034423828\n","Epoch: 0181 cost = 176.038970947\n","Epoch: 0182 cost = 162.393630981\n","Epoch: 0183 cost = 166.870498657\n","Epoch: 0184 cost = 162.770263672\n","Epoch: 0185 cost = 178.763549805\n","Epoch: 0186 cost = 173.253036499\n","Epoch: 0187 cost = 162.275115967\n","Epoch: 0188 cost = 164.513610840\n","Epoch: 0189 cost = 164.351898193\n","Epoch: 0190 cost = 179.090209961\n","Epoch: 0191 cost = 175.785125732\n","Epoch: 0192 cost = 174.084716797\n","Epoch: 0193 cost = 164.963043213\n","Epoch: 0194 cost = 174.700241089\n","Epoch: 0195 cost = 168.201660156\n","Epoch: 0196 cost = 162.340911865\n","Epoch: 0197 cost = 166.778335571\n","Epoch: 0198 cost = 179.504730225\n","Epoch: 0199 cost = 160.920776367\n","Epoch: 0200 cost = 171.103576660\n","Epoch: 0201 cost = 178.410583496\n","Epoch: 0202 cost = 175.039276123\n","Epoch: 0203 cost = 161.366119385\n","Epoch: 0204 cost = 169.695312500\n","Epoch: 0205 cost = 171.313262939\n","Epoch: 0206 cost = 163.624694824\n","Epoch: 0207 cost = 168.306198120\n","Epoch: 0208 cost = 167.605560303\n","Epoch: 0209 cost = 166.662460327\n","Epoch: 0210 cost = 166.239166260\n","Epoch: 0211 cost = 174.240493774\n","Epoch: 0212 cost = 164.016754150\n","Epoch: 0213 cost = 155.108871460\n","Epoch: 0214 cost = 180.326950073\n","Epoch: 0215 cost = 176.575317383\n","Epoch: 0216 cost = 154.722930908\n","Epoch: 0217 cost = 163.592819214\n","Epoch: 0218 cost = 183.848266602\n","Epoch: 0219 cost = 162.441131592\n","Epoch: 0220 cost = 170.569763184\n","Epoch: 0221 cost = 167.931884766\n","Epoch: 0222 cost = 174.684387207\n","Epoch: 0223 cost = 167.922210693\n","Epoch: 0224 cost = 161.002502441\n","Epoch: 0225 cost = 154.439025879\n","Epoch: 0226 cost = 152.774688721\n","Epoch: 0227 cost = 175.659759521\n","Epoch: 0228 cost = 160.521804810\n","Epoch: 0229 cost = 163.428894043\n","Epoch: 0230 cost = 174.863677979\n","Epoch: 0231 cost = 163.909988403\n","Epoch: 0232 cost = 148.462127686\n","Epoch: 0233 cost = 167.162704468\n","Epoch: 0234 cost = 152.225494385\n","Epoch: 0235 cost = 162.081527710\n","Epoch: 0236 cost = 158.461410522\n","Epoch: 0237 cost = 158.145629883\n","Epoch: 0238 cost = 154.484893799\n","Epoch: 0239 cost = 173.543228149\n","Epoch: 0240 cost = 160.242462158\n","Epoch: 0241 cost = 163.875854492\n","Epoch: 0242 cost = 150.379913330\n","Epoch: 0243 cost = 155.945648193\n","Epoch: 0244 cost = 171.396987915\n","Epoch: 0245 cost = 171.089050293\n","Epoch: 0246 cost = 159.674407959\n","Epoch: 0247 cost = 163.161422729\n","Epoch: 0248 cost = 159.498168945\n","Epoch: 0249 cost = 161.593963623\n","Epoch: 0250 cost = 174.189849854\n","Epoch: 0251 cost = 164.886627197\n","Epoch: 0252 cost = 156.426727295\n","Epoch: 0253 cost = 158.598190308\n","Epoch: 0254 cost = 158.329833984\n","Epoch: 0255 cost = 160.072982788\n","Epoch: 0256 cost = 161.296966553\n","Epoch: 0257 cost = 160.350219727\n","Epoch: 0258 cost = 145.604461670\n","Epoch: 0259 cost = 149.571426392\n","Epoch: 0260 cost = 160.760147095\n","Epoch: 0261 cost = 182.357208252\n","Epoch: 0262 cost = 160.269012451\n","Epoch: 0263 cost = 155.999542236\n","Epoch: 0264 cost = 169.576232910\n","Epoch: 0265 cost = 166.586303711\n","Epoch: 0266 cost = 160.627838135\n","Epoch: 0267 cost = 156.331741333\n","Epoch: 0268 cost = 164.323333740\n","Epoch: 0269 cost = 153.400405884\n","Epoch: 0270 cost = 147.312072754\n","Epoch: 0271 cost = 159.758438110\n","Epoch: 0272 cost = 150.577011108\n","Epoch: 0273 cost = 146.041625977\n","Epoch: 0274 cost = 150.342956543\n","Epoch: 0275 cost = 152.712738037\n","Epoch: 0276 cost = 157.478195190\n","Epoch: 0277 cost = 152.812622070\n","Epoch: 0278 cost = 158.984344482\n","Epoch: 0279 cost = 160.716445923\n","Epoch: 0280 cost = 147.842834473\n","Epoch: 0281 cost = 161.405563354\n","Epoch: 0282 cost = 159.233840942\n","Epoch: 0283 cost = 159.027359009\n","Epoch: 0284 cost = 156.010864258\n","Epoch: 0285 cost = 146.571563721\n","Epoch: 0286 cost = 159.383666992\n","Epoch: 0287 cost = 155.010345459\n","Epoch: 0288 cost = 141.928939819\n","Epoch: 0289 cost = 152.100311279\n","Epoch: 0290 cost = 152.457855225\n","Epoch: 0291 cost = 154.585769653\n","Epoch: 0292 cost = 151.065948486\n","Epoch: 0293 cost = 173.142837524\n","Epoch: 0294 cost = 152.190917969\n","Epoch: 0295 cost = 163.400405884\n","Epoch: 0296 cost = 171.354019165\n","Epoch: 0297 cost = 166.074081421\n","Epoch: 0298 cost = 155.023330688\n","Epoch: 0299 cost = 162.657501221\n","Epoch: 0300 cost = 144.666305542\n","Learning finished\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"AHccupIjbm4g","colab_type":"text"},"source":["\n","\n","### test 셋도 Scaler를 통해 transofrom을 해준다.\n","\n","\n","\n","1.   train 셋의 transfrom:   **fit을 사용**\n","- Scaler.fit_transform(x_train_data)\n","- fit -> train 데이터셋을 통해서 어떤 방식으로 scaler를 할지 계산을 구한다는 의미.\n","\n","2. test셋의 transform: **그냥 transform만**\n","-  Scaler.transform(x_test_data) \n","- fit_transform(train)을 통해 구해진 scaler 방식을 test셋에 적용하겠다는 의미. "]},{"cell_type":"code","metadata":{"id":"z_SQp0GxdJfc","colab_type":"code","colab":{}},"source":["# Test the model using test sets\n","with torch.no_grad():\n","\n","  x_test_data=test_data.loc[:,:]\n","  x_test_data=np.array(x_test_data)\n","  x_test_data = Scaler.transform(x_test_data)         # test 셋도 Scaler를 통해 transofrom을 해준다.      \n","  x_test_data=torch.from_numpy(x_test_data).float().to(device)\n","\n","  prediction = model(x_test_data)\n","\n","# argmax를 하지 않는다. predication자체가 예측값이 되기 때문에."],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mZK0F3SY0m8y","colab_type":"code","colab":{}},"source":["correct_prediction = prediction.cpu().numpy().reshape(-1,1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qz8oLaz50or_","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":399},"executionInfo":{"status":"ok","timestamp":1592217626460,"user_tz":-540,"elapsed":5480,"user":{"displayName":"Juhee","photoUrl":"","userId":"14369088239483395487"}},"outputId":"c9435d53-1cfb-4d0d-fd60-4e601ea6d2cd"},"source":["submit=pd.read_csv('sample_submission.csv')\n","submit"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>Price</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2.0</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>3.0</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>4.0</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>162</th>\n","      <td>162.0</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th>163</th>\n","      <td>163.0</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th>164</th>\n","      <td>164.0</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th>165</th>\n","      <td>165.0</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th>166</th>\n","      <td>166.0</td>\n","      <td>1.0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>167 rows × 2 columns</p>\n","</div>"],"text/plain":["        id  Price\n","0      0.0    1.0\n","1      1.0    1.0\n","2      2.0    1.0\n","3      3.0    1.0\n","4      4.0    1.0\n","..     ...    ...\n","162  162.0    1.0\n","163  163.0    1.0\n","164  164.0    1.0\n","165  165.0    1.0\n","166  166.0    1.0\n","\n","[167 rows x 2 columns]"]},"metadata":{"tags":[]},"execution_count":279}]},{"cell_type":"code","metadata":{"id":"REN-barMGaQg","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":399},"executionInfo":{"status":"ok","timestamp":1592217626461,"user_tz":-540,"elapsed":5444,"user":{"displayName":"Juhee","photoUrl":"","userId":"14369088239483395487"}},"outputId":"3a4123a9-be91-4cc1-a2a9-ecd650f18a6e"},"source":["for i in range(len(correct_prediction)):\n","  submit['Price'][i]=correct_prediction[i].item()\n","\n","submit"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>Price</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0.0</td>\n","      <td>25.778234</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1.0</td>\n","      <td>14.252911</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2.0</td>\n","      <td>15.069804</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>3.0</td>\n","      <td>25.778234</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>4.0</td>\n","      <td>15.069804</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>162</th>\n","      <td>162.0</td>\n","      <td>25.778234</td>\n","    </tr>\n","    <tr>\n","      <th>163</th>\n","      <td>163.0</td>\n","      <td>15.069804</td>\n","    </tr>\n","    <tr>\n","      <th>164</th>\n","      <td>164.0</td>\n","      <td>25.778234</td>\n","    </tr>\n","    <tr>\n","      <th>165</th>\n","      <td>165.0</td>\n","      <td>25.778234</td>\n","    </tr>\n","    <tr>\n","      <th>166</th>\n","      <td>166.0</td>\n","      <td>25.608477</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>167 rows × 2 columns</p>\n","</div>"],"text/plain":["        id      Price\n","0      0.0  25.778234\n","1      1.0  14.252911\n","2      2.0  15.069804\n","3      3.0  25.778234\n","4      4.0  15.069804\n","..     ...        ...\n","162  162.0  25.778234\n","163  163.0  15.069804\n","164  164.0  25.778234\n","165  165.0  25.778234\n","166  166.0  25.608477\n","\n","[167 rows x 2 columns]"]},"metadata":{"tags":[]},"execution_count":280}]},{"cell_type":"code","metadata":{"id":"SKu0HYd70tNp","colab_type":"code","colab":{}},"source":["submit.to_csv('baseline.csv',index=False,header=True)\n","\n","# !kaggle competitions submit -c 2020aiboston -f baseline.csv -m \"3layer dropout, 13>512>1\""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lc4m7_pLbxG_","colab_type":"text"},"source":["### Evaluation: RMSE\n","- 해당 regression문제의 Evaluation은 RMSE 이다. (분류문제는 accuracy를 사용)\n","- 0에 가까운 값이 이상적인 값(답)이다."]},{"cell_type":"code","metadata":{"id":"3ru-4NG-Gouz","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]}]}